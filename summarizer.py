from openai import OpenAI
import os


# Prefer env var; fallback to config.py if present
try:
    from config import OPENAI_API_KEY as CONFIG_KEY
except Exception:
    CONFIG_KEY = None

def _get_api_key() -> str:
    """
    Returns the OpenAI API key.
    """
    key = os.getenv("OPENAI_API_KEY") or CONFIG_KEY
    if not key:
        raise RuntimeError("OPENAI_API_KEY is not set (env or config.py).")
    return key


# -----------------------------
# Input loading (TXT / JSON only)
# -----------------------------
def load_medical_file(file_path: str) -> str:
    """
    Reads a medical text file (.txt) and returns its content as a string.
    """
    if not file_path.lower().endswith(".txt"):
        raise ValueError("Only .txt files are supported.")
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read()



# -----------------------------
# Prompt builder 
# -----------------------------
def create_prompt(text: str) -> str:
    
    return f"""
×§×œ×˜ (×˜×§×¡×˜ ××§×•×¨×™):
{text}

×”×•×¨××•×ª:
××ª×” ××¡×›× ×¨×©×•××” ×¨×¤×•××™×ª ×¢×‘×•×¨ ×¤×¨××“×™×§ ×‘×©×˜×—. ×”×¤×œ×˜ ×—×™×™×‘ ×œ×”×™×•×ª ×§×¦×¨, ×¨×©×™××ª×™ ×•×‘×¨Ö¾×¡×¨×™×§×”.
×—×œ×§ ××ª ×”×¤×œ×˜ ×œ×©×œ×•×©×” ××§×˜×¢×™× ×‘×“×™×•×§, ×‘×¡×“×¨ ×”×‘×: ğŸŸ¥, ğŸŸ§, ğŸŸ©.
×‘×›×œ ××§×˜×¢ ×”×¦×’ ××¨×‘×¢ ×ª×ª×™Ö¾×§×˜×’×•×¨×™×•×ª ×§×‘×•×¢×•×ª:
- ××œ×¨×’×™×•×ª/×¨×’×™×©×•×™×•×ª
- ××—×œ×•×ª ×¨×§×¢
- ×ª×¨×•×¤×•×ª ×§×‘×•×¢×•×ª
- ×”× ×—×™×•×ª/××’×‘×œ×•×ª

×›×œ×œ×™× ××—×™×™×‘×™×:
- ×›×ª×•×‘ × ×§×•×“×•×ª ×‘×œ×‘×“ (bullet points), ×œ×œ× ×¤×¡×§××•×ª ×—×•×¤×©×™×•×ª.
- ××œ ×ª××¦×™× ×¢×•×‘×“×•×ª. ×× ××™×“×¢ ×—×¡×¨ â€“ ×›×ª×•×‘ "××™×Ÿ ××™×“×¢".
- ×©××•×¨ ×¢×œ ×¡×“×¨ ×”×›×•×ª×¨×•×ª ×•×”×ª×ª×™Ö¾×›×•×ª×¨×•×ª ×‘×“×™×•×§ ×›×¤×™ ×©××•×¤×™×¢ ×œ×”×œ×Ÿ.

×¤×œ×˜ (×‘×“×™×•×§ ×‘××‘× ×” ×”×‘×):

ğŸŸ¥ ×’×•×¨××™ ×¡×™×›×•×Ÿ ××™×™×“×™×™×
- ××œ×¨×’×™×•×ª/×¨×’×™×©×•×™×•×ª:
  - ××™×Ÿ ××™×“×¢
- ××—×œ×•×ª ×¨×§×¢:
  - ××™×Ÿ ××™×“×¢
- ×ª×¨×•×¤×•×ª ×§×‘×•×¢×•×ª:
  - ××™×Ÿ ××™×“×¢
- ×”× ×—×™×•×ª/××’×‘×œ×•×ª:
  - ××™×Ÿ ××™×“×¢

ğŸŸ§ ×”×™×¡×˜×•×¨×™×” ×¨×¤×•××™×ª ×¨×œ×•×•× ×˜×™×ª
- ××œ×¨×’×™×•×ª/×¨×’×™×©×•×™×•×ª:
  - ××™×Ÿ ××™×“×¢
- ××—×œ×•×ª ×¨×§×¢:
  - ××™×Ÿ ××™×“×¢
- ×ª×¨×•×¤×•×ª ×§×‘×•×¢×•×ª:
  - ××™×Ÿ ××™×“×¢
- ×”× ×—×™×•×ª/××’×‘×œ×•×ª:
  - ××™×Ÿ ××™×“×¢

ğŸŸ© ××™×“×¢ ×›×œ×œ×™
- ××œ×¨×’×™×•×ª/×¨×’×™×©×•×™×•×ª:
  - ××™×Ÿ ××™×“×¢
- ××—×œ×•×ª ×¨×§×¢:
  - ××™×Ÿ ××™×“×¢
- ×ª×¨×•×¤×•×ª ×§×‘×•×¢×•×ª:
  - ××™×Ÿ ××™×“×¢
- ×”× ×—×™×•×ª/××’×‘×œ×•×ª:
  - ××™×Ÿ ××™×“×¢
""".rstrip()


# -----------------------------
# GPT call 
# -----------------------------
def summarize_medical_file(file_path: str,
                           model_name: str,
                           simulation: bool = False) -> str:
    """
    Sends the medical file content to GPT and returns a structured summary.
    If simulation=True, returns a deterministic mock without API calls.
    """
    text = load_medical_file(file_path)
    prompt = create_prompt(text)

    if simulation:
        # Deterministic mock to test formatting without API
        return (
            "ğŸŸ¥ ×’×•×¨××™ ×¡×™×›×•×Ÿ ××™×™×“×™×™×\n"
            "- ××œ×¨×’×™×•×ª/×¨×’×™×©×•×™×•×ª:\n  - ××œ×¨×’×™×” ×œ×¤× ×™×¦×™×œ×™×Ÿ\n"
            "- ××—×œ×•×ª ×¨×§×¢:\n  - ××™×Ÿ ××™×“×¢\n"
            "- ×ª×¨×•×¤×•×ª ×§×‘×•×¢×•×ª:\n  - ××™×Ÿ ××™×“×¢\n"
            "- ×”× ×—×™×•×ª/××’×‘×œ×•×ª:\n  - ××™×Ÿ ×œ×”×›× ×™×¡ ×¢×™×¨×•×™ ×‘×™×“ ×©×××œ\n\n"
            "ğŸŸ§ ×”×™×¡×˜×•×¨×™×” ×¨×¤×•××™×ª ×¨×œ×•×•× ×˜×™×ª\n"
            "- ××œ×¨×’×™×•×ª/×¨×’×™×©×•×™×•×ª:\n  - ×¨×’×™×©×•×ª ×œ×™×•×“\n"
            "- ××—×œ×•×ª ×¨×§×¢:\n  - ×¡×•×›×¨×ª ×¡×•×’ 2\n"
            "- ×ª×¨×•×¤×•×ª ×§×‘×•×¢×•×ª:\n  - ××˜×¤×•×¨××™×Ÿ\n"
            "- ×”× ×—×™×•×ª/××’×‘×œ×•×ª:\n  - ××™×Ÿ ××™×“×¢\n\n"
            "ğŸŸ© ××™×“×¢ ×›×œ×œ×™\n"
            "- ××œ×¨×’×™×•×ª/×¨×’×™×©×•×™×•×ª:\n  - ××™×Ÿ ××™×“×¢\n"
            "- ××—×œ×•×ª ×¨×§×¢:\n  - ×›××‘×™ ×’×‘ ×›×¨×•× ×™×™×\n"
            "- ×ª×¨×•×¤×•×ª ×§×‘×•×¢×•×ª:\n  - ××™×Ÿ ××™×“×¢\n"
            "- ×”× ×—×™×•×ª/××’×‘×œ×•×ª:\n  - × ×¢×–×¨ ×‘×§×‘×™×™×"
        )

    client = OpenAI(api_key=_get_api_key())

    
    response = client.chat.completions.create(
        model=model_name,               # LLM version (e.g., "gpt-4o-mini" for cheap & reliable)
        messages=[                      # Conversation turns; here we only send a single user turn
            {"role": "user", "content": prompt}
        ],
        temperature=0.0,                # 0.0 = deterministic output; critical for strict clinical formatting
        top_p=1.0,                      # Keep 1.0 when temperature=0. it means consider all kind of outputs.
        frequency_penalty=0.0,          # Discourage repetition (0.0 keeps neutral behavior)
        presence_penalty=0.0,           # Encourage novelty (0.0 prevents drifting from the structure)
        max_tokens=1000                 # Upper bound for output length/cost control
    )

    summary = response.choices[0].message.content
    return summary


# -----------------------------
# Save output
# -----------------------------
def save_summary_to_file(summary: str, output_path: str = "summary_output.txt"):
    """
    Saves the summary to a text file (overwrites if exists).
    """
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(summary)
